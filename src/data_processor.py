"""
Processeur principal pour l'enrichissement des donnÃ©es d'entreprises
"""

import pandas as pd
import logging
from typing import Dict, List, Optional, Any
from .insee_client import INSEEClient

logger = logging.getLogger(__name__)

class DataProcessor:
    """Processeur pour enrichir des donnÃ©es d'entreprises avec l'API INSEE"""
    
    def __init__(self, insee_client: INSEEClient):
        """
        Initialise le processeur
        
        Args:
            insee_client: Instance du client INSEE configurÃ©
        """
        self.client = insee_client
        self.duplicate_cache = {}
        
    def process_companies(self, df: pd.DataFrame, 
                         company_col: str, 
                         size_col: str = None,
                         batch_size: int = 100) -> pd.DataFrame:
        """
        Traite un DataFrame d'entreprises pour enrichissement INSEE
        
        Args:
            df: DataFrame avec les donnÃ©es d'entreprises
            company_col: Nom de la colonne contenant les noms d'entreprises
            size_col: Nom de la colonne contenant la taille d'entreprise (optionnel)
            batch_size: Taille des lots pour sauvegarde intermÃ©diaire
            
        Returns:
            DataFrame enrichi avec donnÃ©es INSEE
        """
        logger.info(f"Traitement de {len(df)} entreprises avec optimisation des doublons")
        
        # Analyse des doublons
        duplicates_analysis = self._analyze_duplicates(df, company_col)
        logger.info(f"Doublons dÃ©tectÃ©s: {duplicates_analysis['total_duplicates']} lignes dupliquÃ©es")
        
        results = []
        cache_hits = 0
        api_calls = 0
        
        for idx, row in df.iterrows():
            company_name = str(row[company_col]).strip()
            size_original = str(row[size_col]).strip() if size_col and pd.notna(row[size_col]) else 'Non spÃ©cifiÃ©'
            
            # VÃ©rification cache doublons
            if company_name in self.duplicate_cache:
                logger.debug(f"[{idx+1}/{len(df)}] {company_name} - ðŸ’¾ CACHE HIT")
                result_data = self.duplicate_cache[company_name].copy()
                cache_hits += 1
            else:
                # Nouvelle recherche
                logger.info(f"[{idx+1}/{len(df)}] {company_name} - ðŸ” Nouvelle recherche API...")
                insee_data = self.client.search_company(company_name)
                api_calls += 1
                
                if insee_data:
                    result_data = {
                        'Organisation_Original': company_name,
                        'Taille_Original': size_original,
                        'Statut_Recherche': 'TrouvÃ©',
                        **insee_data
                    }
                    logger.info(f"   âœ… TrouvÃ©: {insee_data.get('Denomination_INSEE', 'N/A')}")
                else:
                    result_data = {
                        'Organisation_Original': company_name,
                        'Taille_Original': size_original,
                        'Statut_Recherche': 'Non trouvÃ©',
                        'SIREN': None,
                        'SIRET': None,
                        'Denomination_INSEE': None,
                        'Effectifs_Description': 'Non spÃ©cifiÃ©',
                        'Effectifs_Numeric': None
                    }
                    logger.warning(f"   âŒ Non trouvÃ©")
                
                # Mise en cache
                self.duplicate_cache[company_name] = result_data.copy()
                
                # Pause entre requÃªtes dÃ©jÃ  gÃ©rÃ©e dans le client
            
            results.append(result_data)
            
            # Progress info
            if (idx + 1) % 10 == 0:
                progress = (idx + 1) / len(df) * 100
                eta_minutes = (len(df) - idx - 1) * 4 / 60  # Estimation basÃ©e sur 4s par requÃªte
                logger.info(f"   â±ï¸  Progression: {progress:.1f}% | ETA: {eta_minutes:.0f}min")
        
        # Statistiques finales
        found = len([r for r in results if r.get('Statut_Recherche') == 'TrouvÃ©'])
        logger.info(f"\nðŸ“Š STATISTIQUES:")
        logger.info(f"   ðŸ¢ Entreprises traitÃ©es: {len(results)}")
        logger.info(f"   âœ… TrouvÃ©es: {found} ({found/len(results)*100:.1f}%)")
        logger.info(f"   ðŸ”— Appels API: {api_calls}")
        logger.info(f"   ðŸ’¾ Cache hits: {cache_hits}")
        logger.info(f"   âš¡ Ã‰conomie: {cache_hits} requÃªtes Ã©vitÃ©es!")
        
        return pd.DataFrame(results)
    
    def _analyze_duplicates(self, df: pd.DataFrame, company_col: str) -> Dict[str, Any]:
        """Analyse les doublons dans le dataset"""
        logger.info("ðŸ” Analyse des doublons...")
        
        # Compter les occurrences de chaque entreprise
        counts = df[company_col].value_counts()
        duplicates = counts[counts > 1]
        
        total_companies = len(df)
        unique_companies = len(counts)
        duplicate_companies = len(duplicates)
        total_duplicates = duplicates.sum() - duplicate_companies  # Nombre de lignes en trop
        
        logger.info(f"ðŸ“Š ANALYSE DU DATASET:")
        logger.info(f"   ðŸ“„ Total lignes: {total_companies}")
        logger.info(f"   ðŸ¢ Entreprises uniques: {unique_companies}")
        logger.info(f"   ðŸ”„ Entreprises dupliquÃ©es: {duplicate_companies}")
        logger.info(f"   âš ï¸  Lignes dupliquÃ©es: {total_duplicates}")
        logger.info(f"   ðŸ’¡ Ã‰conomie possible: {total_duplicates} requÃªtes Ã©vitÃ©es!")
        
        if duplicate_companies > 0:
            logger.info(f"\nðŸ”¢ TOP 10 doublons:")
            for company, count in duplicates.head(10).items():
                logger.info(f"   {company}: {count} fois")
        
        return {
            'total_companies': total_companies,
            'unique_companies': unique_companies,
            'duplicate_companies': duplicate_companies,
            'total_duplicates': total_duplicates,
            'most_duplicated': list(duplicates.head(10).items())
        }